# Machine_Learning_Insurance_Risk
3 Different Machine Learning Methods Performed
# Task
  Support Vector Machines (SVM), Artificial Neural Networks (ANN), and Random Forests are three common and powerful supervised machine learning methods.  Each have their strengths and weaknesses depending on the data being used and the goal of the researcher.  In this exercise we use Python in Jupyter to upload the data, preprocess the data, and compare the results for each of the three methods.
  The data file used is auto_imports_1985.csv, which is data on auto imports from a database in 1985.  Some exploratory data shows the file has 205 records and 26 columns, with no missing values. There are 26 variables, both continuous and categorical.  The categorical variables are already encoded so any necessary label encoding steps are avoided.  The target variable is symboling, which represents the insurance risk factor.  Values of the categorical target variable are [-2, -1, 0, 1, 2, 3], -2 being least risky and 3 being most risky.
# Support Vector Machine (SVM)
  SVM is a supervised machine learning method shown to be effective at producing non-linear boundaries with high dimensional datasets.  The SVM’s decision boundaries are formed by emphasizing the closest instances to the boundary from the different classes and creating a separation with the largest margin between those close instances.  These close instances are called support vectors.  The SVM also uses a non-linear function called a kernel that expands the original data points to high dimensions so that a boundary can separate the classes.  The most common kernel is the Radial Basis Function (RBF), which is what is used in this exercise. 
	Gamma and C are two other parameters that must be carefully tuned for optimal results.  A high C value tells the algorithm to prioritize classifying each individual training point, creating a complex boundary but increasing chance of overfitting.  A low C value tries to classify all training points and restricts the boundary, decreasing chances of overfitting.  The gamma parameter sets the reach of influence of the instances.  If the gamma is low then the reach is far for the instances, then the boundary will not have to be close to compensate, so it will be a restricted boundary.  If the gamma is high then the reach is short for the instances, then the boundary will be closer to the instances and it will be more complex.  The C and gamma values first start with value of 1.0 for each, and the accuracy score was 78% on the test data.  C value was raised to 2.0 and gamma to 1.5 and the accuracy remained unchanged at 78%.  Then C=1.5 and gamma=1.0 resulted in 80% accuracy.  After trying different variations, the parameters landed at C=2.0, gamma=0.75, accuracy is 85%.
# Artificial Neural Network
  ANN is a supervised machine learning method that has had a recent reemergence particularly with large, high dimensional datasets.  ANN’s name comes from its algorithm that reflects the human brain.  The end algorithm is made up of layers of perceptrons with weighted nodes that classify the final result.  The model is built first with the input layer which is the original attributes and the outer layer which is the final prediction.  In between them are hidden layer(s) whose number is chosen by the researcher.  Each hidden layer has a number of nodes, chosen by the researcher, with random weights assigned and connect to the nodes or features to nearby layers.  
	After the initial run, the model then corrects itself based on the error of the result.  The process is reversed from the output layer back to the hidden layers.  Weights are adjusted for all the nodes to minimize the error.  This process is known as backpropagation.  The process is iterated enough times to get the minimum error of the model.  The number of iterations and rate at which the weights are changed (gradient) is also chosen by the researcher. 
	Three hidden layers are chosen.  The initial run has 30 nodes per hidden layer and produces a test accuracy of 68%.  Nodes were then increased to 50 each with accuracy of 73%.  Nodes are then decreased to 40 and ends with accuracy result of 82%.  I left the parameters the same and repeated the model and the accuracy went from 82% to 75%.  This is due to the random placement of weights in the beginning coupled with the small sample size.   The results are left at the last run, 75% test accuracy.
# Random Forest Classifier
  Random Forest is an ensemble method designed to reduce noise or error and thus increasing prediction accuracy on new data.  The logic behind Random Forests is that individual Decision Trees are relatively good at making predictions but tend to have some error in different areas that lead to overfitting.  A Random Forest model is a random group of many decision trees where the results are aggregated, and error is averaged out and reduced overall.  This averaging of noise is called bagging. 
	The first part of the Random Forest algorithm the researcher decides the number of decision trees in the model.  Then samples are randomly chosen from the dataset with replacement.  This is called bootstrapping.  When the algorithm is running, each node chooses the best splitting feature from the feature subset offered.  The number of features per subset is less than the total features so randomization of features occur.  An object is classified by the most votes among the decision trees in the random forest model.  For example, if 10 decision trees are used in a model and 4 of the trees classify it as Class A and 3 classify it as B, and 3 classify it as C, then the model classifies the object as Class A.  
	100 decision trees are used in the Random Forest model.  The max number of features offered at each node is set to default, which in this exercise it is the square root of the number of features = 5.  The result (Figure 4) is 83% test accuracy.  The number of decision trees were increased to 500 to see if the accuracy improved.  It did not, so the model was left to 100 decision trees.  
# Summary
  The SVM provided the highest test accuracy results at 85% with Random Forest coming in close at 83%.  Although SVM provided the highest accuracy score, it doesn’t necessarily provide the best understanding.  SVM are often hard to understand why a particular prediction was made and even harder to explain it to a non-technical group.  We saw this first hand in this exercise as the 85% accuracy was arrived at through trial and error with tuning the C and gamma parameters. One has to question if adding features and/or samples to the data would produce the same accuracy.   Random Forests, on the other hand, do not require much tuning of parameters.  Though somewhat complex, they are easier to explain and understand than SVM.  Due to the complex nature of SVMs and the variance of results when tuning the parameters, I would say Random Forests provided the best understanding and as a researcher it would be tempting to go with the reliability and simplicity of Random Forests at the cost of 2 percentage points.  
	Not only did ANN produce the lowest test accuracy results at 75%, but the randomness of the results makes it an unreliable model.  It varied by seven percentage points from iteration to iteration without changing any parameters.  ANN is also one of the more complex models to understand and explain.  The low results, complexity, and volatility make the ANN model in this exercise provide the least understanding.  This is most likely due to the small sample size of 205*.80(training data) = 164 to train the model.  ANN often outperform other models with enough data and time to carefully tune the parameters.  In this case, there wasn't nearly enough data to train the ANN model without error.  


